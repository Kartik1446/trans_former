{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:31:57.678329Z","iopub.execute_input":"2025-12-22T10:31:57.679023Z","iopub.status.idle":"2025-12-22T10:31:57.916858Z","shell.execute_reply.started":"2025-12-22T10:31:57.678991Z","shell.execute_reply":"2025-12-22T10:31:57.915949Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# Tiny Shakespear\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:31:57.918226Z","iopub.execute_input":"2025-12-22T10:31:57.918609Z","iopub.status.idle":"2025-12-22T10:31:58.598976Z","shell.execute_reply.started":"2025-12-22T10:31:57.918591Z","shell.execute_reply":"2025-12-22T10:31:58.597912Z"}},"outputs":[{"name":"stdout","text":"--2025-12-22 10:31:57--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt.1’\n\ninput.txt.1         100%[===================>]   1.06M  6.29MB/s    in 0.2s    \n\n2025-12-22 10:31:58 (6.29 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\nmax_iters = 3000\neval_interval = 300\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel(vocab_size)\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:31:58.599991Z","iopub.execute_input":"2025-12-22T10:31:58.600255Z","iopub.status.idle":"2025-12-22T10:32:05.112139Z","shell.execute_reply.started":"2025-12-22T10:31:58.600228Z","shell.execute_reply":"2025-12-22T10:32:05.110783Z"}},"outputs":[{"name":"stdout","text":"step 0: train loss 4.7305, val loss 4.7241\nstep 300: train loss 2.8110, val loss 2.8249\nstep 600: train loss 2.5434, val loss 2.5682\nstep 900: train loss 2.4932, val loss 2.5088\nstep 1200: train loss 2.4863, val loss 2.5035\nstep 1500: train loss 2.4665, val loss 2.4921\nstep 1800: train loss 2.4683, val loss 2.4936\nstep 2100: train loss 2.4696, val loss 2.4846\nstep 2400: train loss 2.4638, val loss 2.4879\nstep 2700: train loss 2.4738, val loss 2.4911\n\nod nos CAy go ghanoray t, co haringoudrou clethe k,LARof fr werar,\nIs fa!\n\n\nThilemel cia h hmboomyorarifrcitheviPO, tle dst f qur'dig t cof boddo y t o ar pileas h mo wierl t,\nS:\nSTENENEat I athe thounomy tinrent distesisanimald 3I: eliento ald, avaviconofrisist me Busarend un'soto vat s k,\nSBRI he the f wendleindd t acoe ts ansu, thy ppr h.QULY:\nKIIsqu pr odEd ch,\nAPrnes ouse bll owhored miner t ooon'stoume bupromo! fifoveghind hiarnge s.\nMI aswimy or m, wardd tw'To tee abifewoetsphin sed The a\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"# Mathematical Trick in self-attention models","metadata":{}},{"cell_type":"code","source":"# considering the following toy example:\ntorch.manual_seed(1337)\nB,T,C= 4,8,2 #batch, time, channels(information at each point in the sequence)\nx= torch.randn(B,T,C)\nx.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:32:05.113082Z","iopub.execute_input":"2025-12-22T10:32:05.113298Z","iopub.status.idle":"2025-12-22T10:32:05.120750Z","shell.execute_reply.started":"2025-12-22T10:32:05.113283Z","shell.execute_reply":"2025-12-22T10:32:05.120061Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 2])"},"metadata":{}}],"execution_count":62},{"cell_type":"markdown","source":"##### tokens should not interact with other future tokens, it should only talk to the previous tokens so the information flows from previous context to the current time stamp.\n","metadata":{}},{"cell_type":"code","source":"# We want x[b,t]=mean_{i<=t} x[b,i]\n#bow means bag of words\nxbow= torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev= x[b, :t+1] #(t,C)\n        xbow[b,t]= torch.mean(xprev, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:35:39.367217Z","iopub.execute_input":"2025-12-22T10:35:39.367530Z","iopub.status.idle":"2025-12-22T10:35:39.373642Z","shell.execute_reply.started":"2025-12-22T10:35:39.367511Z","shell.execute_reply":"2025-12-22T10:35:39.372833Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"wei= torch.tril(torch.ones(T,T))\nwei= wei/wei.sum(1, keepdim=True)\nxbow2= wei @ x # (B,T,T) @ (B,T,C) ---> (B,T,C)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:37:51.575365Z","iopub.execute_input":"2025-12-22T10:37:51.575632Z","iopub.status.idle":"2025-12-22T10:37:51.580336Z","shell.execute_reply.started":"2025-12-22T10:37:51.575617Z","shell.execute_reply":"2025-12-22T10:37:51.579640Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"torch.allclose(xbow, xbow2, atol=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:36:41.163185Z","iopub.execute_input":"2025-12-22T10:36:41.163425Z","iopub.status.idle":"2025-12-22T10:36:41.170280Z","shell.execute_reply.started":"2025-12-22T10:36:41.163400Z","shell.execute_reply":"2025-12-22T10:36:41.169536Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"tril = torch.tril(torch.ones(T,T))\nwei= torch.zeros((T,T))\nwei= tril.masked_fill(tril == 0, float('-inf'))\nwei= F.softmax(wei, dim=-1)\nxbow3= wei @ x \ntorch.allclose(xbow, xbow3, atol=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:56:36.999353Z","iopub.execute_input":"2025-12-22T10:56:36.999618Z","iopub.status.idle":"2025-12-22T10:56:37.007653Z","shell.execute_reply.started":"2025-12-22T10:56:36.999600Z","shell.execute_reply":"2025-12-22T10:56:37.006946Z"}},"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":94},{"cell_type":"markdown","source":"This code computes a **bag-of-words (BoW) style running average** of the input tensor `x`. For each batch index `b` and time step `t`, the value `xbow[b, t]` is set to the mean of all vectors `x[b, i]` where `i ≤ t`. In other words, each position contains the average of the current and all previous token representations. This corresponds to the definition  \n\n$$\nx[b,t] = \\text{mean}_{   i \\le } x[b,i]\n$$\n\nand is referred to as a *bag-of-words* representation because it aggregates information from previous tokens without preserving their order beyond inclusion.\n","metadata":{"execution":{"iopub.status.busy":"2025-12-22T09:59:30.061330Z","iopub.execute_input":"2025-12-22T09:59:30.061580Z","iopub.status.idle":"2025-12-22T09:59:30.066676Z","shell.execute_reply.started":"2025-12-22T09:59:30.061565Z","shell.execute_reply":"2025-12-22T09:59:30.065827Z"}}},{"cell_type":"code","source":"x[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:32:05.173789Z","iopub.execute_input":"2025-12-22T10:32:05.174105Z","iopub.status.idle":"2025-12-22T10:32:05.191022Z","shell.execute_reply.started":"2025-12-22T10:32:05.174087Z","shell.execute_reply":"2025-12-22T10:32:05.189984Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.1808, -0.0700],\n        [-0.3596, -0.9152],\n        [ 0.6258,  0.0255],\n        [ 0.9545,  0.0643],\n        [ 0.3612,  1.1679],\n        [-1.3499, -0.5102],\n        [ 0.2360, -0.2398],\n        [-0.9211,  1.5433]])"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"xbow[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:32:05.191858Z","iopub.execute_input":"2025-12-22T10:32:05.192128Z","iopub.status.idle":"2025-12-22T10:32:05.209473Z","shell.execute_reply.started":"2025-12-22T10:32:05.192114Z","shell.execute_reply":"2025-12-22T10:32:05.208485Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.1808, -0.0700],\n        [-0.0894, -0.4926],\n        [ 0.1490, -0.3199],\n        [ 0.3504, -0.2238],\n        [ 0.3525,  0.0545],\n        [ 0.0688, -0.0396],\n        [ 0.0927, -0.0682],\n        [-0.0341,  0.1332]])"},"metadata":{}}],"execution_count":67},{"cell_type":"markdown","source":"`xbow[0]` is printing with the average of all the past context, <br>Whereas `x[0]` is only considering the present element.\n\nBut I don't think any of these is efficient.(#freewill)\n","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(42)\na= torch.ones(3,3)\nb= torch.randint(0,10,(3,2)).float()\nc= a@b\nprint(f'a={a}')\nprint()\nprint(f'b= {b}')\nprint()\nprint(f'c={c}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:32:05.210235Z","iopub.execute_input":"2025-12-22T10:32:05.210470Z","iopub.status.idle":"2025-12-22T10:32:05.228357Z","shell.execute_reply.started":"2025-12-22T10:32:05.210452Z","shell.execute_reply":"2025-12-22T10:32:05.227562Z"}},"outputs":[{"name":"stdout","text":"a=tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\n\nb= tensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n\nc=tensor([[14., 16.],\n        [14., 16.],\n        [14., 16.]])\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"torch.manual_seed(42)\na= torch.tril(torch.ones(3,3))\nb= torch.randint(0,10,(3,2)).float()\nc= a@b\nprint(f'a={a}')\nprint()\nprint(f'b= {b}')\nprint()\nprint(f'c={c}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:32:05.228981Z","iopub.execute_input":"2025-12-22T10:32:05.229274Z","iopub.status.idle":"2025-12-22T10:32:05.251207Z","shell.execute_reply.started":"2025-12-22T10:32:05.229256Z","shell.execute_reply":"2025-12-22T10:32:05.250136Z"}},"outputs":[{"name":"stdout","text":"a=tensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n\nb= tensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n\nc=tensor([[ 2.,  7.],\n        [ 8., 11.],\n        [14., 16.]])\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"torch.manual_seed(42)\na= torch.tril(torch.ones(3,3))\na= a/torch.sum(a,1, keepdim=True)\nb= torch.randint(0,10,(3,2)).float()\nc= a@b\nprint(f'a={a}')\nprint()\nprint(f'b= {b}')\nprint()\nprint(f'c={c}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T10:32:05.252046Z","iopub.execute_input":"2025-12-22T10:32:05.252277Z","iopub.status.idle":"2025-12-22T10:32:05.269339Z","shell.execute_reply.started":"2025-12-22T10:32:05.252257Z","shell.execute_reply":"2025-12-22T10:32:05.268513Z"}},"outputs":[{"name":"stdout","text":"a=tensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n\nb= tensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n\nc=tensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
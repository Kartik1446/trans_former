{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:28.482272Z","iopub.execute_input":"2025-12-24T14:01:28.482717Z","iopub.status.idle":"2025-12-24T14:01:28.884096Z","shell.execute_reply.started":"2025-12-24T14:01:28.482677Z","shell.execute_reply":"2025-12-24T14:01:28.882876Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Tiny Shakespear\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:28.885642Z","iopub.execute_input":"2025-12-24T14:01:28.886195Z","iopub.status.idle":"2025-12-24T14:01:29.241335Z","shell.execute_reply.started":"2025-12-24T14:01:28.886161Z","shell.execute_reply":"2025-12-24T14:01:29.240061Z"}},"outputs":[{"name":"stdout","text":"--2025-12-24 14:01:28--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n\n2025-12-24 14:01:29 (29.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\nmax_iters = 3000\neval_interval = 300\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel(vocab_size)\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:29.243035Z","iopub.execute_input":"2025-12-24T14:01:29.243422Z","iopub.status.idle":"2025-12-24T14:01:51.261402Z","shell.execute_reply.started":"2025-12-24T14:01:29.243368Z","shell.execute_reply":"2025-12-24T14:01:51.260433Z"}},"outputs":[{"name":"stdout","text":"step 0: train loss 4.7305, val loss 4.7241\nstep 300: train loss 2.8110, val loss 2.8249\nstep 600: train loss 2.5434, val loss 2.5682\nstep 900: train loss 2.4932, val loss 2.5088\nstep 1200: train loss 2.4863, val loss 2.5035\nstep 1500: train loss 2.4665, val loss 2.4921\nstep 1800: train loss 2.4683, val loss 2.4936\nstep 2100: train loss 2.4696, val loss 2.4846\nstep 2400: train loss 2.4638, val loss 2.4879\nstep 2700: train loss 2.4738, val loss 2.4911\n\nMARI he avayokis erceller thour d, myono thishe me tord se by he me, Forder anen: at trselorinjulour t yoru thrd wo ththathy IUShe bavidelanoby man ond be jus as g e atot Meste hrle s, ppat t JLENCOLIUS:\nOppid tes d s o ged moer y pevehear soue maramapay fo t: bueyo malalyo!\nDuir.\nFl ke it I t l o'ddre d ondu s?\ncr, havetrathackes w.\nPUpee meshancun, hrendspouthoulouren whel's'sesoread pe, s whure our heredinsethes; sedsend r lo pamit,\nQUMIVIVIOfe m ne RDINid we tr ort; t:\nMINENXI l dintandore r\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Mathematical Trick in self-attention models","metadata":{}},{"cell_type":"code","source":"# considering the following toy example:\ntorch.manual_seed(1337)\nB,T,C= 4,8,2 #batch, time, channels(information at each point in the sequence)\nx= torch.randn(B,T,C)\nx.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:51.263628Z","iopub.execute_input":"2025-12-24T14:01:51.264065Z","iopub.status.idle":"2025-12-24T14:01:51.275952Z","shell.execute_reply.started":"2025-12-24T14:01:51.264036Z","shell.execute_reply":"2025-12-24T14:01:51.275066Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 2])"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"##### tokens should not interact with other future tokens, it should only talk to the previous tokens so the information flows from previous context to the current time stamp.\n","metadata":{}},{"cell_type":"code","source":"# We want x[b,t]=mean_{i<=t} x[b,i]\n#bow means bag of words\nxbow= torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev= x[b, :t+1] #(t,C)\n        xbow[b,t]= torch.mean(xprev, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:51.277042Z","iopub.execute_input":"2025-12-24T14:01:51.277414Z","iopub.status.idle":"2025-12-24T14:01:51.294750Z","shell.execute_reply.started":"2025-12-24T14:01:51.277367Z","shell.execute_reply":"2025-12-24T14:01:51.293633Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"wei= torch.tril(torch.ones(T,T))\nwei= wei/wei.sum(1, keepdim=True)\nxbow2= wei @ x # (B,T,T) @ (B,T,C) ---> (B,T,C)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:51.296213Z","iopub.execute_input":"2025-12-24T14:01:51.296611Z","iopub.status.idle":"2025-12-24T14:01:51.317829Z","shell.execute_reply.started":"2025-12-24T14:01:51.296573Z","shell.execute_reply":"2025-12-24T14:01:51.316350Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"torch.allclose(xbow, xbow2, atol=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:51.319306Z","iopub.execute_input":"2025-12-24T14:01:51.319655Z","iopub.status.idle":"2025-12-24T14:01:51.338666Z","shell.execute_reply.started":"2025-12-24T14:01:51.319625Z","shell.execute_reply":"2025-12-24T14:01:51.337800Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"tril = torch.tril(torch.ones(T,T))\nwei= torch.zeros((T,T))\nwei= tril.masked_fill(tril == 0, float('-inf'))\nwei= F.softmax(wei, dim=-1)\nxbow3= wei @ x \ntorch.allclose(xbow, xbow3, atol=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:51.339827Z","iopub.execute_input":"2025-12-24T14:01:51.340149Z","iopub.status.idle":"2025-12-24T14:01:51.365644Z","shell.execute_reply.started":"2025-12-24T14:01:51.340121Z","shell.execute_reply":"2025-12-24T14:01:51.364451Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"This code computes a **bag-of-words (BoW) style running average** of the input tensor `x`. For each batch index `b` and time step `t`, the value `xbow[b, t]` is set to the mean of all vectors `x[b, i]` where `i ≤ t`. In other words, each position contains the average of the current and all previous token representations. This corresponds to the definition  \n\n$$\nx[b,t] = \\text{mean}_{   i \\le } x[b,i]\n$$\n\nand is referred to as a *bag-of-words* representation because it aggregates information from previous tokens without preserving their order beyond inclusion.\n","metadata":{"execution":{"iopub.status.busy":"2025-12-22T09:59:30.061330Z","iopub.execute_input":"2025-12-22T09:59:30.061580Z","iopub.status.idle":"2025-12-22T09:59:30.066676Z","shell.execute_reply.started":"2025-12-22T09:59:30.061565Z","shell.execute_reply":"2025-12-22T09:59:30.065827Z"}}},{"cell_type":"code","source":"x[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:51.367202Z","iopub.execute_input":"2025-12-24T14:01:51.367516Z","iopub.status.idle":"2025-12-24T14:01:51.406818Z","shell.execute_reply.started":"2025-12-24T14:01:51.367486Z","shell.execute_reply":"2025-12-24T14:01:51.405676Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.1808, -0.0700],\n        [-0.3596, -0.9152],\n        [ 0.6258,  0.0255],\n        [ 0.9545,  0.0643],\n        [ 0.3612,  1.1679],\n        [-1.3499, -0.5102],\n        [ 0.2360, -0.2398],\n        [-0.9211,  1.5433]])"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"xbow[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:51.409591Z","iopub.execute_input":"2025-12-24T14:01:51.409926Z","iopub.status.idle":"2025-12-24T14:01:51.426279Z","shell.execute_reply.started":"2025-12-24T14:01:51.409898Z","shell.execute_reply":"2025-12-24T14:01:51.425290Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.1808, -0.0700],\n        [-0.0894, -0.4926],\n        [ 0.1490, -0.3199],\n        [ 0.3504, -0.2238],\n        [ 0.3525,  0.0545],\n        [ 0.0688, -0.0396],\n        [ 0.0927, -0.0682],\n        [-0.0341,  0.1332]])"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"`xbow[0]` is printing with the average of all the past context, <br>Whereas `x[0]` is only considering the present element.\n\nBut I don't think any of these is efficient.(#freewill)\n","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(42)\na= torch.ones(3,3)\nb= torch.randint(0,10,(3,2)).float()\nc= a@b\nprint(f'a={a}')\nprint()\nprint(f'b= {b}')\nprint()\nprint(f'c={c}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:51.427596Z","iopub.execute_input":"2025-12-24T14:01:51.428038Z","iopub.status.idle":"2025-12-24T14:01:51.459135Z","shell.execute_reply.started":"2025-12-24T14:01:51.428005Z","shell.execute_reply":"2025-12-24T14:01:51.457579Z"}},"outputs":[{"name":"stdout","text":"a=tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\n\nb= tensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n\nc=tensor([[14., 16.],\n        [14., 16.],\n        [14., 16.]])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"torch.manual_seed(42)\na= torch.tril(torch.ones(3,3))\nb= torch.randint(0,10,(3,2)).float()\nc= a@b\nprint(f'a={a}')\nprint()\nprint(f'b= {b}')\nprint()\nprint(f'c={c}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:51.460311Z","iopub.execute_input":"2025-12-24T14:01:51.460612Z","iopub.status.idle":"2025-12-24T14:01:51.471945Z","shell.execute_reply.started":"2025-12-24T14:01:51.460570Z","shell.execute_reply":"2025-12-24T14:01:51.470822Z"}},"outputs":[{"name":"stdout","text":"a=tensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n\nb= tensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n\nc=tensor([[ 2.,  7.],\n        [ 8., 11.],\n        [14., 16.]])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"torch.manual_seed(42)\na= torch.tril(torch.ones(3,3))\na= a/torch.sum(a,1, keepdim=True)\nb= torch.randint(0,10,(3,2)).float()\nc= a@b\nprint(f'a={a}')\nprint()\nprint(f'b= {b}')\nprint()\nprint(f'c={c}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:01:51.473369Z","iopub.execute_input":"2025-12-24T14:01:51.473767Z","iopub.status.idle":"2025-12-24T14:01:51.497883Z","shell.execute_reply.started":"2025-12-24T14:01:51.473728Z","shell.execute_reply":"2025-12-24T14:01:51.496673Z"}},"outputs":[{"name":"stdout","text":"a=tensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n\nb= tensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n\nc=tensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx= torch.randn(B,T,C)\n\ntril = torch.tril(torch.ones(T,T))\nwei= torch.zeros((T,T))\nwei= wei.masked_fill(tril ==0, float('-inf'))\nwei= F.softmax(wei, dim=-1)\nout= wei@x\n\nout.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:05:07.491238Z","iopub.execute_input":"2025-12-24T14:05:07.491579Z","iopub.status.idle":"2025-12-24T14:05:07.502925Z","shell.execute_reply.started":"2025-12-24T14:05:07.491526Z","shell.execute_reply":"2025-12-24T14:05:07.501982Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 32])"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx= torch.randn(B,T,C)\n\n# let's see a single head perform self-attention\nhead_size= 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C,head_size,bias=False)\nk= key(x) #(B,T, 16)\nq=query(x) #(B,T, 16)\nv= value(x)\n\nwei= q @ k.transpose(-2,-1) #(B,T,16)@(B,16,T ) ---> (B,T,T)\n\ntril = torch.tril(torch.ones(T,T))\n# wei= torch.zeros((T,T))\nwei= wei.masked_fill(tril ==0, float('-inf'))\nwei= F.softmax(wei, dim=-1)\nout= wei@v\n\nout.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:31:56.789474Z","iopub.execute_input":"2025-12-24T14:31:56.790482Z","iopub.status.idle":"2025-12-24T14:31:56.805110Z","shell.execute_reply.started":"2025-12-24T14:31:56.790438Z","shell.execute_reply":"2025-12-24T14:31:56.804085Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 16])"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"wei","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:29:21.841517Z","iopub.execute_input":"2025-12-24T14:29:21.841977Z","iopub.status.idle":"2025-12-24T14:29:21.853046Z","shell.execute_reply.started":"2025-12-24T14:29:21.841941Z","shell.execute_reply":"2025-12-24T14:29:21.851870Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n\n        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n\n        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n\n        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n       grad_fn=<SoftmaxBackward0>)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"wei","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:22:25.361752Z","iopub.execute_input":"2025-12-24T14:22:25.362119Z","iopub.status.idle":"2025-12-24T14:22:25.372432Z","shell.execute_reply.started":"2025-12-24T14:22:25.362088Z","shell.execute_reply":"2025-12-24T14:22:25.371175Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n         [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n         [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n         [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n\n        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.4016, 0.5984, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.3365, 0.2271, 0.4364, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.3019, 0.2060, 0.2899, 0.2022, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.1058, 0.1700, 0.1530, 0.3451, 0.2261, 0.0000, 0.0000, 0.0000],\n         [0.1526, 0.1645, 0.1357, 0.2684, 0.1919, 0.0869, 0.0000, 0.0000],\n         [0.1103, 0.1711, 0.0761, 0.1654, 0.1667, 0.1643, 0.1461, 0.0000],\n         [0.1770, 0.1063, 0.1198, 0.0943, 0.1697, 0.1205, 0.1052, 0.1073]],\n\n        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.4955, 0.5045, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.2861, 0.3657, 0.3483, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.1242, 0.3939, 0.1981, 0.2838, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.3531, 0.1668, 0.1768, 0.1813, 0.1220, 0.0000, 0.0000, 0.0000],\n         [0.1553, 0.1779, 0.1492, 0.1539, 0.1723, 0.1914, 0.0000, 0.0000],\n         [0.0722, 0.1255, 0.1119, 0.1896, 0.1537, 0.1918, 0.1552, 0.0000],\n         [0.1344, 0.1368, 0.0970, 0.1395, 0.1292, 0.1304, 0.0790, 0.1535]],\n\n        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.5351, 0.4649, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.3776, 0.4907, 0.1317, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.3079, 0.2849, 0.2206, 0.1865, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.2074, 0.2611, 0.1368, 0.2071, 0.1876, 0.0000, 0.0000, 0.0000],\n         [0.1733, 0.3004, 0.0656, 0.1682, 0.1669, 0.1255, 0.0000, 0.0000],\n         [0.1216, 0.1213, 0.1416, 0.1119, 0.1439, 0.2213, 0.1383, 0.0000],\n         [0.0925, 0.1598, 0.0945, 0.1355, 0.1356, 0.1086, 0.1185, 0.1548]]],\n       grad_fn=<SoftmaxBackward0>)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"wei[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:29:32.617080Z","iopub.execute_input":"2025-12-24T14:29:32.617432Z","iopub.status.idle":"2025-12-24T14:29:32.626965Z","shell.execute_reply.started":"2025-12-24T14:29:32.617401Z","shell.execute_reply":"2025-12-24T14:29:32.625705Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"## Version 4: Self-Attention (Single Head)\n\nThis code demonstrates a minimal implementation of **single-head causal self-attention**, similar to what is used inside Transformer models.\n\n### Setup\nWe first fix the random seed for reproducibility and define the input dimensions:\n- **B**: batch size  \n- **T**: sequence length (time steps / tokens)  \n- **C**: embedding dimension (channels)\n\nA random input tensor `x` of shape `(B, T, C)` represents a batch of token embeddings.\n\n### Key, Query, and Value Projections\nSelf-attention works by projecting the input embeddings into three separate spaces:\n- **Keys (K)** determine *what information* each token contains\n- **Queries (Q)** determine *what information* each token is looking for\n- **Values (V)** contain the information that will be aggregated\n\nEach projection is implemented as a linear layer mapping from `C` to `head_size`.\n\n### Attention Weights\nWe compute attention scores by taking the dot product between queries and keys:\n\n$$\n\\text{attention} = QK^\\top\n$$\n\n\nThis produces a tensor of shape `(B, T, T)`, where each token attends to every other token in the sequence.\n\n### Causal Masking\nTo prevent tokens from attending to future tokens (important for autoregressive models), a **lower-triangular mask** is applied. Positions above the diagonal are set to `-inf`, ensuring their attention weights become zero after softmax.\n\n### Softmax Normalization\nThe masked attention scores are passed through a softmax so that each row forms a valid probability distribution over past tokens.\n\n### Weighted Sum of Values\nFinally, the attention weights are used to compute a weighted sum of the value vectors:\n\n$$\n\\text{output} = \\text{softmax}(QK^\\top) \\cdot V\n$$\n\nThe resulting tensor `out` has shape `(B, T, head_size)` and represents the output of a single self-attention head.\n","metadata":{}},{"cell_type":"markdown","source":"Notes:\n* Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n* There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n* Each example across batch dimension is of course processed completely independently and never \"talk\" to each other.\n* In an \"encoder\" attention block just delete the single line that does maskingwith `tril`, allowing all tokens to communicate. This block here is caller a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n* \"Self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (eg. an encoder module)\n* \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input q,k are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much.","metadata":{}},{"cell_type":"code","source":"k= torch.randn(B,T, head_size) #(B,T, 16)\nq= torch.randn(B,T, head_size) #(B,T, 16)\nwei= q@k.transpose(-2,-1) #*head_size**-0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T15:12:18.788600Z","iopub.execute_input":"2025-12-24T15:12:18.789017Z","iopub.status.idle":"2025-12-24T15:12:18.795115Z","shell.execute_reply.started":"2025-12-24T15:12:18.788970Z","shell.execute_reply":"2025-12-24T15:12:18.794238Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"k.var()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T15:12:23.797447Z","iopub.execute_input":"2025-12-24T15:12:23.798458Z","iopub.status.idle":"2025-12-24T15:12:23.806482Z","shell.execute_reply.started":"2025-12-24T15:12:23.798419Z","shell.execute_reply":"2025-12-24T15:12:23.805650Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"tensor(1.0449)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"q.var()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T15:12:27.982165Z","iopub.execute_input":"2025-12-24T15:12:27.982473Z","iopub.status.idle":"2025-12-24T15:12:27.990845Z","shell.execute_reply.started":"2025-12-24T15:12:27.982448Z","shell.execute_reply":"2025-12-24T15:12:27.989873Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor(1.0700)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"wei.var()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T15:12:33.623468Z","iopub.execute_input":"2025-12-24T15:12:33.623881Z","iopub.status.idle":"2025-12-24T15:12:33.632194Z","shell.execute_reply.started":"2025-12-24T15:12:33.623849Z","shell.execute_reply":"2025-12-24T15:12:33.631009Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"tensor(17.4690)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"wei= q@k.transpose(-2,-1) *head_size**-0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T15:14:35.013313Z","iopub.execute_input":"2025-12-24T15:14:35.013658Z","iopub.status.idle":"2025-12-24T15:14:35.019904Z","shell.execute_reply.started":"2025-12-24T15:14:35.013629Z","shell.execute_reply":"2025-12-24T15:14:35.019020Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"print(k.var())\nprint(q.var())\nprint(wei.var())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T15:14:35.438095Z","iopub.execute_input":"2025-12-24T15:14:35.438477Z","iopub.status.idle":"2025-12-24T15:14:35.447237Z","shell.execute_reply.started":"2025-12-24T15:14:35.438441Z","shell.execute_reply":"2025-12-24T15:14:35.445986Z"}},"outputs":[{"name":"stdout","text":"tensor(1.0449)\ntensor(1.0700)\ntensor(1.0918)\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.3,0.5]) ,dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T15:15:44.968256Z","iopub.execute_input":"2025-12-24T15:15:44.968629Z","iopub.status.idle":"2025-12-24T15:15:44.977923Z","shell.execute_reply.started":"2025-12-24T15:15:44.968593Z","shell.execute_reply":"2025-12-24T15:15:44.976841Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"tensor([0.1951, 0.1446, 0.2384, 0.1308, 0.2911])"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.3,0.5])*8 ,dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T15:16:09.663348Z","iopub.execute_input":"2025-12-24T15:16:09.663703Z","iopub.status.idle":"2025-12-24T15:16:09.672695Z","shell.execute_reply.started":"2025-12-24T15:16:09.663672Z","shell.execute_reply":"2025-12-24T15:16:09.671656Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"tensor([0.0327, 0.0030, 0.1618, 0.0013, 0.8013])"},"metadata":{}}],"execution_count":42},{"cell_type":"markdown","source":"by multiplying with 8 we just sharepened the variables and it will be wayy too peaky,","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
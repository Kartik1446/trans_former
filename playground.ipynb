{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:14.432296Z","iopub.execute_input":"2025-12-22T09:01:14.432584Z","iopub.status.idle":"2025-12-22T09:01:14.715053Z","shell.execute_reply.started":"2025-12-22T09:01:14.432558Z","shell.execute_reply":"2025-12-22T09:01:14.714450Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Tiny Shakespear\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:14.716593Z","iopub.execute_input":"2025-12-22T09:01:14.717225Z","iopub.status.idle":"2025-12-22T09:01:14.988548Z","shell.execute_reply.started":"2025-12-22T09:01:14.717199Z","shell.execute_reply":"2025-12-22T09:01:14.987841Z"}},"outputs":[{"name":"stdout","text":"--2025-12-22 09:01:14--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n\n2025-12-22 09:01:14 (20.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"with open('input.txt', 'r', encoding= 'utf-8') as f:\n    text= f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:14.989749Z","iopub.execute_input":"2025-12-22T09:01:14.990015Z","iopub.status.idle":"2025-12-22T09:01:14.996213Z","shell.execute_reply.started":"2025-12-22T09:01:14.989989Z","shell.execute_reply":"2025-12-22T09:01:14.995530Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(len(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:14.998168Z","iopub.execute_input":"2025-12-22T09:01:14.998494Z","iopub.status.idle":"2025-12-22T09:01:15.010077Z","shell.execute_reply.started":"2025-12-22T09:01:14.998464Z","shell.execute_reply":"2025-12-22T09:01:15.009345Z"}},"outputs":[{"name":"stdout","text":"1115394\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(text[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:15.010934Z","iopub.execute_input":"2025-12-22T09:01:15.011150Z","iopub.status.idle":"2025-12-22T09:01:15.025408Z","shell.execute_reply.started":"2025-12-22T09:01:15.011130Z","shell.execute_reply":"2025-12-22T09:01:15.024633Z"}},"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# special charactersa\nchars= sorted(list(set(text)))\nvocab_size= len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:15.026310Z","iopub.execute_input":"2025-12-22T09:01:15.026499Z","iopub.status.idle":"2025-12-22T09:01:15.054725Z","shell.execute_reply.started":"2025-12-22T09:01:15.026481Z","shell.execute_reply":"2025-12-22T09:01:15.054090Z"}},"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Resources\nhttps://github.com/openai/tiktoken <br>\nhttps://github.com/google/sentencepiece","metadata":{"execution":{"iopub.status.busy":"2025-12-18T11:50:37.343064Z","iopub.execute_input":"2025-12-18T11:50:37.343862Z","iopub.status.idle":"2025-12-18T11:50:37.351624Z","shell.execute_reply.started":"2025-12-18T11:50:37.343832Z","shell.execute_reply":"2025-12-18T11:50:37.350176Z"}}},{"cell_type":"markdown","source":"# Tiktoken draft (rough)","metadata":{}},{"cell_type":"code","source":"import tiktoken\nenc= tiktoken.get_encoding('gpt2')\nenc.n_vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:15.055670Z","iopub.execute_input":"2025-12-22T09:01:15.056030Z","iopub.status.idle":"2025-12-22T09:01:16.149012Z","shell.execute_reply.started":"2025-12-22T09:01:15.055996Z","shell.execute_reply":"2025-12-22T09:01:16.148266Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"50257"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"enc.encode(\"hIi there\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:16.150083Z","iopub.execute_input":"2025-12-22T09:01:16.150379Z","iopub.status.idle":"2025-12-22T09:01:16.166999Z","shell.execute_reply.started":"2025-12-22T09:01:16.150349Z","shell.execute_reply":"2025-12-22T09:01:16.166219Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[71, 40, 72, 612]"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"enc.decode([71, 4178, 612])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:16.167970Z","iopub.execute_input":"2025-12-22T09:01:16.168202Z","iopub.status.idle":"2025-12-22T09:01:16.173715Z","shell.execute_reply.started":"2025-12-22T09:01:16.168182Z","shell.execute_reply":"2025-12-22T09:01:16.172987Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'hii there'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"enc.encode(\"Kartik\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:16.175769Z","iopub.execute_input":"2025-12-22T09:01:16.175991Z","iopub.status.idle":"2025-12-22T09:01:16.184166Z","shell.execute_reply.started":"2025-12-22T09:01:16.175972Z","shell.execute_reply":"2025-12-22T09:01:16.183426Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[42, 433, 1134]"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"enc.decode([42, 433, 1134])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:16.185056Z","iopub.execute_input":"2025-12-22T09:01:16.185300Z","iopub.status.idle":"2025-12-22T09:01:16.199438Z","shell.execute_reply.started":"2025-12-22T09:01:16.185274Z","shell.execute_reply":"2025-12-22T09:01:16.198732Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'Kartik'"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## Anyways continuing","metadata":{"execution":{"iopub.status.busy":"2025-12-18T11:57:33.177694Z","iopub.execute_input":"2025-12-18T11:57:33.178028Z","iopub.status.idle":"2025-12-18T11:57:33.182737Z","shell.execute_reply.started":"2025-12-18T11:57:33.178007Z","shell.execute_reply":"2025-12-18T11:57:33.181661Z"}}},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi= {ch:i for i, ch in enumerate(chars)}\nitos= {i:ch for i, ch in enumerate(chars)}                 \nencode = lambda s:[stoi[c] for c in s] # ecnoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string.\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:16.200235Z","iopub.execute_input":"2025-12-22T09:01:16.200428Z","iopub.status.idle":"2025-12-22T09:01:16.215419Z","shell.execute_reply.started":"2025-12-22T09:01:16.200410Z","shell.execute_reply":"2025-12-22T09:01:16.214698Z"}},"outputs":[{"name":"stdout","text":"[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch\ndata= torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:16.216291Z","iopub.execute_input":"2025-12-22T09:01:16.216539Z","iopub.status.idle":"2025-12-22T09:01:20.563615Z","shell.execute_reply.started":"2025-12-22T09:01:16.216508Z","shell.execute_reply":"2025-12-22T09:01:20.562907Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"##### this all the vectors you are seeing is the translation of the first 1000 chracters in tiny shapespear dataset","metadata":{"execution":{"iopub.status.busy":"2025-12-18T12:11:47.138317Z","iopub.execute_input":"2025-12-18T12:11:47.139640Z","iopub.status.idle":"2025-12-18T12:11:47.147520Z","shell.execute_reply.started":"2025-12-18T12:11:47.139606Z","shell.execute_reply":"2025-12-18T12:11:47.146056Z"}}},{"cell_type":"code","source":"# Let's now split up the data into train and validation sets\nn= int(0.9*len(data)) # first 90% will be train, rest validation\ntrain_data = data[:n]\nval_data= data[n:] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:20.564610Z","iopub.execute_input":"2025-12-22T09:01:20.565171Z","iopub.status.idle":"2025-12-22T09:01:20.569121Z","shell.execute_reply.started":"2025-12-22T09:01:20.565146Z","shell.execute_reply":"2025-12-22T09:01:20.568380Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"block_size= 8\ntrain_data[:block_size+1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:20.570256Z","iopub.execute_input":"2025-12-22T09:01:20.570814Z","iopub.status.idle":"2025-12-22T09:01:20.588359Z","shell.execute_reply.started":"2025-12-22T09:01:20.570781Z","shell.execute_reply":"2025-12-22T09:01:20.587830Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"we want to train it on the series of most probable words, so we rather train the data on chunks instead of the whole at the same time. this way the learning rate also goes up.","metadata":{}},{"cell_type":"code","source":"x= train_data[:block_size]\ny= train_data[1:block_size+1]\nfor t in range(block_size):\n    context= x[:t+1]\n    target= y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:20.589932Z","iopub.execute_input":"2025-12-22T09:01:20.590186Z","iopub.status.idle":"2025-12-22T09:01:20.602033Z","shell.execute_reply.started":"2025-12-22T09:01:20.590167Z","shell.execute_reply":"2025-12-22T09:01:20.601401Z"}},"outputs":[{"name":"stdout","text":"when input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"that's how a transformer works. ","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(1337)\nbatch_size= 4 # independent sequences in parallel processing\nblock_size= 8 # max context length for predictions\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split== 'train' else val_data\n    ix= torch.randint(len(data) - block_size, (batch_size,))\n    x= torch.stack([data[i:i+block_size] for i in ix])\n    y= torch.stack([data[i+1:i+block_size+1]for i in ix])\n    return x,y\n\nxb, yb= get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context= xb[b,:t+1]\n        target= yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:20.603029Z","iopub.execute_input":"2025-12-22T09:01:20.603233Z","iopub.status.idle":"2025-12-22T09:01:20.639734Z","shell.execute_reply.started":"2025-12-22T09:01:20.603214Z","shell.execute_reply":"2025-12-22T09:01:20.639119Z"}},"outputs":[{"name":"stdout","text":"inputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"print(xb) # input in the transformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:20.640493Z","iopub.execute_input":"2025-12-22T09:01:20.640924Z","iopub.status.idle":"2025-12-22T09:01:20.645995Z","shell.execute_reply.started":"2025-12-22T09:01:20.640894Z","shell.execute_reply":"2025-12-22T09:01:20.645374Z"}},"outputs":[{"name":"stdout","text":"tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table =nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        #idx and targets are both (B,T) tensor of integers\n        logits= self.token_embedding_table(idx) #(B,T,C)\n\n        if targets is None:\n            loss= None\n        else:\n            B,T,C= logits.shape\n            logits=logits.view(B*T,C)\n            targets= targets.view(B*T)\n            loss= F.cross_entropy(logits, targets)\n        \n        return logits, loss\n\n    def generate(self, idx, max_tokens):\n         #idx is (B,T) array of indices in the current context\n         for _ in range(max_tokens):\n             # get the predictions\n             logits, loss = self(idx)\n             # focus only on the last time step\n             logits = logits[:,-1,:] #becomes (B,C)\n             probs= F.softmax(logits, dim=-1) #(B,C)\n             #sample from the distribution\n             idx_next= torch.multinomial(probs, num_samples=1) #(B,1)\n             # append sampled index to the running sequence\n             idx= torch.cat((idx, idx_next), dim=1) #(B, T+1)\n         return idx\n         \nm= BigramLanguageModel(vocab_size)\nlogits, loss= m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_tokens=100)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:20.646858Z","iopub.execute_input":"2025-12-22T09:01:20.647230Z","iopub.status.idle":"2025-12-22T09:01:20.735721Z","shell.execute_reply.started":"2025-12-22T09:01:20.647201Z","shell.execute_reply":"2025-12-22T09:01:20.735149Z"}},"outputs":[{"name":"stdout","text":"torch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward0>)\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"the expected loss was supposed to be `-ln(1/65)` = `4.1743872699`","metadata":{}},{"cell_type":"markdown","source":"A bigram language model is a simple statistical tool in NLP that predicts the next word in a sequence based only on the immediately preceding word, using conditional probabilities from text analysis (a corpus) to find common word pairs (bigrams) like \"quick brown\" or \"machine learning\". It works by counting how often words follow each other, assuming the future depends only on the present (Markov assumption). ","metadata":{}},{"cell_type":"code","source":"# PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:01:20.736459Z","iopub.execute_input":"2025-12-22T09:01:20.736798Z","iopub.status.idle":"2025-12-22T09:01:23.536927Z","shell.execute_reply.started":"2025-12-22T09:01:20.736776Z","shell.execute_reply":"2025-12-22T09:01:23.536313Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"batch_size=32\nfor steps in range(100):\n    #sample a batch of data\n    xb,yb= get_batch('train')\n\n    # evaluate the loss\n    logits, loss= m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:08:20.217483Z","iopub.execute_input":"2025-12-22T09:08:20.218267Z","iopub.status.idle":"2025-12-22T09:08:20.381514Z","shell.execute_reply.started":"2025-12-22T09:08:20.218235Z","shell.execute_reply":"2025-12-22T09:08:20.380900Z"}},"outputs":[{"name":"stdout","text":"2.425917863845825\n","output_type":"stream"}],"execution_count":286},{"cell_type":"code","source":"print(decode(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_tokens=300)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:08:20.382577Z","iopub.execute_input":"2025-12-22T09:08:20.383217Z","iopub.status.idle":"2025-12-22T09:08:20.428127Z","shell.execute_reply.started":"2025-12-22T09:08:20.383192Z","shell.execute_reply":"2025-12-22T09:08:20.427585Z"}},"outputs":[{"name":"stdout","text":"\nHe CHedey t,\nOUCERGBYeange ftheay s:\nThesteexpin, cadrinor hulou eath u g.\nKELUCindnd thideicofa ththe,\nThadve oot hest hyoonece ililil be y knten nof coure endit ay meseswitey;\nCLI?\nLEXE: nd e be Y dyowenof wies thof bat nixiseas he\nSTUSTousin ed mabede meck tof haimus pofr'dowintuakncl:\nFr;\nMENGLA\n","output_type":"stream"}],"execution_count":287}]}